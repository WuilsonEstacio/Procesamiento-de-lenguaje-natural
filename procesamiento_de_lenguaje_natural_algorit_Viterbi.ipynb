{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "procesamiento de lenguaje natural algorit Viterbi.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kWuuOIOESJ5yGM0oP212QRPCgJvUITUw",
      "authorship_tag": "ABX9TyOPJMPTGhJ2RNQ/2DYqsKMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WuilsonEstacio/Procesamiento-de-lenguaje-natural/blob/main/procesamiento_de_lenguaje_natural_algorit_Viterbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZEyQmVubee"
      },
      "source": [
        "# Algoritmo de Viterbi\n",
        "\n",
        "Permite hallar la secuencia más probable de estados ocultos que produce una secuencia observada de sucesos, especialmente en el contexto de fuentes de información de Márkov y modelos ocultos de Márkov. Se aplica de forma general en la descodificación de códigos convolucionales usados en redes de telefonía celular digital GSM y CDMA, módems de líneas conmutadas, satélites, comunicaciones espaciales y redes inalámbricas IEEE 802.11. También se usa en reconocimiento del habla, síntesis de habla, diarización, búsqueda de palabras clave, lingüística computacional y bioinformática.\n",
        " https://es.wikipedia.org/wiki/Algoritmo_de_Viterbi \n",
        "\n",
        "https://github.com/rb-one/Curso_Algoritmos_Clasificacion_Texto/blob/main/Notas/notes.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMybtstZZkU4"
      },
      "source": [
        "Estos pasos dependen de algoritmo creado en el cuaderno anterior.\n",
        "\n",
        "procesamiento de lenguaje natural Etiquetado.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEdMVwiuGTWL"
      },
      "source": [
        "# instalacion de dependencias previas\n",
        "!pip install conllu\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQlh6bp1ydXS"
      },
      "source": [
        "# Carga del modelo HMM previamente entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pzeo7hTud5e"
      },
      "source": [
        "# cargamos las probabilidades del modelo HMM del codigo de emision y transmision dadas en \n",
        "# procesamiento de lenguaje natural Etiquetado.ipynb\n",
        "import numpy as np\n",
        "# se crean bariables para almacenas probabilidades de emision y transision \n",
        "transitionProbdict = np.load('transitionHMM.npy', allow_pickle='TRUE').item() # allow_pickle Permitir guardar matrices de objetos utilizando encurtidos Python\n",
        "emissionProbdict = np.load('emissionHMM.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYx-3Gcb4-cu"
      },
      "source": [
        "#  muestra las probabilidades condicionales\n",
        "transitionProbdict\n",
        "# emissionProbdict\n",
        "# con esto ya hemos cargado el modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As7cTF4Kyg90"
      },
      "source": [
        "# identificamos las categorias gramaticales 'upos' unicas en el corpus\n",
        "# utilizamos set para evitar que se repitan las categorias gramaticales\n",
        "stateSet = set([w.split('|')[1] for w in list(emissionProbdict.keys())]) # escogemos w.split('|') para solo selecional la categoria gramatical de cada uno de los elementos\n",
        "stateSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZb4-U_QyhFA"
      },
      "source": [
        "# enumeramos las categorias con numeros para asignar a \n",
        "# las columnas de la matriz de Viterbi\n",
        "\n",
        "# esto es un diccionario con relaciones llave valor donde las llaves \n",
        "# van ha ser las categorias gramaticales y los valores el numero de la clumna que le va a corresponder en la matriz\n",
        "tagStateDict = {} \n",
        "for i, state in enumerate(stateSet): \n",
        "  tagStateDict[state] = i\n",
        "tagStateDict\n",
        "# con esto tenemos un diccionario que relaciona cada categoria gramatical\n",
        "# con un indice que dice cual seria la fila que le corresponde dentro de la matriz de viterbi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf-eutBTynSn"
      },
      "source": [
        "# Distribucion inicial de estados latentes\n",
        "\n",
        "los estados latentes son la primera palabra que hay en cada frase dl corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTbxM1A7yhLc"
      },
      "source": [
        "# Calculamos distribución inicial de estados\n",
        "initTagStateProb = {} # \\rho_i^{(0)} creamos este diccionario que son las probabilidades de encontrar cierta categoria gramatical al inicio de la frase\n",
        "from conllu import parse_incr \n",
        "wordList = []\n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
        "count = 0 # cuenta la longitud del corpus\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  count += 1 # incrementamos el contador\n",
        "  tag = tokenlist[0]['upos'] # cojo el primer elemento con la catagoria gramatical con el upos\n",
        "  if tag in initTagStateProb.keys():\n",
        "    initTagStateProb[tag] += 1\n",
        "  else:\n",
        "    initTagStateProb[tag] = 1\n",
        "\n",
        "\n",
        "for key in initTagStateProb.keys(): \n",
        "  initTagStateProb[key] /= count # a cada elemento del dicionatio lo divido por el total de frases que hay\n",
        "\n",
        "initTagStateProb \n",
        "# esto nos devuelve las probabilidades por cada una de las categorias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhcY099865p"
      },
      "source": [
        "# para calcular la suma de las probabilidades que tiene que ser 1\n",
        "sum(initTagStateProb.values()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4-kt4MNyhSL"
      },
      "source": [
        "# verificamos que la suma de las probabilidades que deben sumar  1 (100%)\n",
        "np.array([initTagStateProb[k] for k in initTagStateProb.keys()]).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlhzJSQWy0M9"
      },
      "source": [
        "# Construccion Algoritmo de Viterbi\n",
        "\n",
        "Dada una secuencia de palabras $\\{p_1, p_2, \\dots, p_n \\}$, y un conjunto de categorias gramaticales dadas por la convención `upos`, se considera la matriz de probabilidades de Viterbi así:\n",
        "\n",
        "$$\n",
        "\\begin{array}{c c}\n",
        "\\begin{array}{c c c c}\n",
        "\\text{ADJ} \\\\\n",
        "\\text{ADV}\\\\\n",
        "\\text{PRON} \\\\\n",
        "\\vdots \\\\\n",
        "{}\n",
        "\\end{array} \n",
        "&\n",
        "\\left[\n",
        "\\begin{array}{c c c c}\n",
        "\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n",
        "\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\ \n",
        "\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n",
        "\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n",
        "p_1 & p_2 & \\dots & p_n \n",
        "\\end{array}\n",
        "\\right] \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Donde las probabilidades de la primera columna (para una categoria $i$) están dadas por: \n",
        "\n",
        "$$\n",
        "\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{probabilidad inicial}} \\times \\underbrace{P(p_1 \\vert i)}_{\\text{emisión}}\n",
        "$$\n",
        "\n",
        "luego, para la segunda columna (dada una categoria $j$) serán: \n",
        "\n",
        "$$\n",
        "\\nu_2(j) = \\max_i \\{ \\nu_1(i) \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_2 \\vert j)}_{\\text{emisión}} \\}\n",
        "$$\n",
        "\n",
        "así, en general las probabilidades para la columna $t$ estarán dadas por: \n",
        "\n",
        "$$\n",
        "\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{estado anterior}} \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_t \\vert j)}_{\\text{emisión}} \\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wtViFSAyhYg"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe98tDIQyhfO"
      },
      "source": [
        "# debemos construir la funcion porque el resultado de la funcio sera devolverme la matriz de Vitervi \n",
        "# que corresponde a la secuencia de palabras que le vamos a pasar como argumento de entrada a ala funcion\n",
        "def ViterbiMatrix(secuencia, transitionProbdict=transitionProbdict,\n",
        "                  emissionProbdict=emissionProbdict, tagStateDict=tagStateDict,\n",
        "                  initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia) # creamos el toquenizador para una secuencia toquenizada\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # 17 porque upos tiene 17 categorias, matriz(filas,columnas)\n",
        "\n",
        "  # inicialización primera columna\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key] # generemos una variable de la fila corresondiente a la etiqueta\n",
        "    word_tag = seq[0].lower()+'|'+key # asignamos de la secuencia e palabras el primer elemento y sumo la key que coresponde a al etiqueta \n",
        "    if word_tag in emissionProbdict.keys(): # creamos los elementos de la matriz\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag]\n",
        "\n",
        "  # computo de todas las probabilidades de las siguientes columnas\n",
        "  for col in range(1, len(seq)): # col para recores las columnas\n",
        "    for key in tagStateDict.keys(): # hago un for sobre cada llave o categorias gramatica\n",
        "      tag_row = tagStateDict[key] # asigno la fila relacionada con la categoria gramatical\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys(): # para ver si la etiqueta palabra esta en emission p..\n",
        "        # miramos estados de la col anterior\n",
        "        possible_probs = [] # para ello  creamos lista basia que la llenaremos con un for\n",
        "        for key2 in tagStateDict.keys(): \n",
        "          tag_row2 = tagStateDict[key2] # creamos la fila asociada a key2\n",
        "          tag_prevtag = key+'|'+key2 # consideramos las probabilidades de transiscion y eso se hace ju tando la llave actual con la anterior\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0: # aqui y en las 2 siguentes lineas consideramos la ultima parte \n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        viterbiProb[tag_row, col] = max(possible_probs)  # ahora debemos escoger el maximo de todos esos elementos\n",
        "  \n",
        "  return viterbiProb\n",
        "\n",
        "matrix = ViterbiMatrix('El mundo es pequeño')\n",
        "matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI2Fw7P-zEtd"
      },
      "source": [
        "# funcion para calcular las etiquetas\n",
        "def ViterbiTags(secuencia, transitionProbdict=transitionProbdict, emissionProbdict=emissionProbdict, \n",
        "            tagStateDict=tagStateDict, initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia)\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # upos tiene 17 categorias\n",
        "\n",
        "  # inicialización primera columna\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key]\n",
        "    word_tag = seq[0].lower()+'|'+key\n",
        "    if word_tag in emissionProbdict.keys():\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag]\n",
        "\n",
        "  # computo de las siguientes columnas\n",
        "  for col in range(1, len(seq)):\n",
        "    for key in tagStateDict.keys():\n",
        "      tag_row = tagStateDict[key]\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys():\n",
        "        # miramos estados de la col anterior\n",
        "        possible_probs = []\n",
        "        for key2 in tagStateDict.keys(): \n",
        "          tag_row2 = tagStateDict[key2]\n",
        "          tag_prevtag = key+'|'+key2\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0:\n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        viterbiProb[tag_row, col] = max(possible_probs)\n",
        "# hasta aqui repetimos el paso anterior\n",
        "\n",
        "# ahora agregaremos un paso extra donde realizaremos\n",
        "    # contruccion de secuencia de tags o etiquetas\n",
        "    res = [] # contruimos lista basia llamada res\n",
        "    for i, p in enumerate(seq): # creamos for para recorrer cada palabra de la secuencia y ademas la vamos enumerando las palabras\n",
        "      for tag in tagStateDict.keys(): #para recorrer todos los tags para que pueda ver la probabilidad de que esa palabra este asociada a lasetiquetas\n",
        "        if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n",
        "          res.append((p, tag)) # este paso es para  agregar lo anterior a la lista\n",
        "                                # aqui tendreos una lista de palabra etiqueta\n",
        "  return res\n",
        "\n",
        "ViterbiTags('el mundo es muy pequeño')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUF9Mb_hzE0t"
      },
      "source": [
        "ViterbiTags('estos instrumentos han de rasgar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15O1rz-GzKdJ"
      },
      "source": [
        "# Entrenamiento directo de HMM con NLTK\n",
        "clase en python (NLTK) de HMM: https://www.nltk.org/_modules/nltk/tag/hmm.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16XgPjiOzXRq"
      },
      "source": [
        "# Ejemplo con el Corpus Treebank en ingles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK-GYJzfzQKy"
      },
      "source": [
        "#@title ejemplo con el Corpus Treebank en ingles\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank\n",
        "train_data = treebank.tagged_sents()[:3900] # esto es asi para coger una parte de los datos para entrenamiento y la otra parte para prueba\n",
        "# y escogemos hasta la sentencia 3900 y dejamos el resto para test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmR-36Njze4a"
      },
      "source": [
        "# estructura de la data de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5GWLGyozQaj"
      },
      "source": [
        "#@title estructura de la data de entrenamiento\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht-JacXSzjML"
      },
      "source": [
        "# HMM pre-construido en NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxYRqk4RzQkh"
      },
      "source": [
        "#@title HMM pre-construido en NLTK\n",
        "from nltk.tag import hmm # cargamos clase para poder utilizar modelo\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data) # parapedir que sea un entrenamiento supervisado\n",
        "# tagger = hmm.HiddenMarkovModelTagger.train(train_data) \n",
        "tagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSGfzp_zQ2i"
      },
      "source": [
        "tagger.tag(\"Pierre Vinken will get old\".split()) # aquii estamos tokenizando con un .split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0xxmC4DzoEd"
      },
      "source": [
        "#@title training accuracy\n",
        "# para ver el porcentaje de evaluacion del modelos, osea su presicion sobre el conjunto de entrenamiento\n",
        "tagger.evaluate(treebank.tagged_sents()[:3900])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9dL9E-9zuiL"
      },
      "source": [
        "Ejercicio de práctica\n",
        "Objetivo: Entrena un HMM usando la clase hmm.HiddenMarkovModelTrainer() sobre el dataset UD_Spanish_AnCora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgsCIfBxz1fO"
      },
      "source": [
        "1. **Pre-procesamiento:** En el ejemplo anterior usamos el dataset en ingles `treebank`, el cual viene con una estructura diferente a la de `AnCora`, en esta parte escribe código para transformar la estructura de `AnCora` de manera que quede igual al `treebank` que usamos así:\n",
        "\n",
        "$$\\left[ \\left[ (\\text{'El'}, \\text{'DET'}), (\\dots), \\dots\\right], \\left[\\dots \\right] \\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzog7jgazqLA"
      },
      "source": [
        "# desarrolla tu código aquí \n",
        "# Instalamos conllu para leer el corpus\n",
        "!pip install conllu\n",
        "# Bajamos el corpus de AnCora\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git\n",
        "from conllu import parse_incr\n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\") \n",
        "\n",
        "\n",
        "# Hacemos la transformacion del corpus al formato requerido\n",
        "wordList = [] # creamos lista basia\n",
        "for tokenlist in parse_incr(data_file): \n",
        "  wordList2 = []\n",
        "  for token in tokenlist:\n",
        "    tag = token['upos']\n",
        "    valor = token['form']\n",
        "    wordList2.append((valor,tag)) \n",
        "  wordList.append(wordList2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLJhGtcz7aN"
      },
      "source": [
        "2. **Entrenamiento:** Una vez que el dataset esta con la estructura correcta, utiliza la clase `hmm.HiddenMarkovModelTrainer()` para entrenar con el $80 \\%$ del dataset como conjunto de `entrenamiento` y $20 \\%$ para el conjunto de `test`.\n",
        "\n",
        "**Ayuda:** Para la separacion entre conjuntos de entrenamiento y test, puedes usar la funcion de Scikit Learn: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "En este punto el curso de Machine Learning con Scikit Learn es un buen complemento para entender mejor las funcionalidades de Scikit Learn: https://platzi.com/cursos/scikitlearn-ml/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k9_MJCGz2jv"
      },
      "source": [
        "# desarrolla tu código aquí\n",
        "\n",
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI_AVxVoz_Dp"
      },
      "source": [
        "3. **Validación del modelo:** Un vez entrenado el `tagger`, calcula el rendimiento del modelo (usando `tagger.evaluate()`) para los conjuntos de `entrenamiento` y `test`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvmFWGbw0CN1"
      },
      "source": [
        "#desarrolla tu código aquí\n",
        "# Separamos el corpus\n",
        "wordList_train, wordList_test= train_test_split(wordList, test_size=0.20, random_state=42)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(wordList_train)\n",
        "tagger\n",
        "\n",
        "print(tagger.evaluate(wordList_test))\n",
        "print(tagger.evaluate(wordList_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrS_0dFDBnTL"
      },
      "source": [
        "Procesamiento de lenguaje natural\n",
        "kERNEL: es una funcion matematica que toma mediciones que se comportan de manera no lineal"
      ]
    }
  ]
}