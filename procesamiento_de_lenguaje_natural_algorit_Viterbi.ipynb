{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "procesamiento de lenguaje natural algorit Viterbi.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kWuuOIOESJ5yGM0oP212QRPCgJvUITUw",
      "authorship_tag": "ABX9TyNmCXXiRKOxwGvLXbwjP5Z1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WuilsonEstacio/Procesamiento-de-lenguaje-natural/blob/main/procesamiento_de_lenguaje_natural_algorit_Viterbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZEyQmVubee"
      },
      "source": [
        "# Algoritmo de Viterbi\n",
        "\n",
        "Permite hallar la secuencia más probable de estados ocultos que produce una secuencia observada de sucesos, especialmente en el contexto de fuentes de información de Márkov y modelos ocultos de Márkov. Se aplica de forma general en la descodificación de códigos convolucionales usados en redes de telefonía celular digital GSM y CDMA, módems de líneas conmutadas, satélites, comunicaciones espaciales y redes inalámbricas IEEE 802.11. También se usa en reconocimiento del habla, síntesis de habla, diarización, búsqueda de palabras clave, lingüística computacional y bioinformática.\n",
        " https://es.wikipedia.org/wiki/Algoritmo_de_Viterbi \n",
        "\n",
        "https://github.com/rb-one/Curso_Algoritmos_Clasificacion_Texto/blob/main/Notas/notes.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMybtstZZkU4"
      },
      "source": [
        "Estos pasos dependen de algoritmo creado en el cuaderno anterior.\n",
        "\n",
        "procesamiento de lenguaje natural Etiquetado.ipynb\n",
        "\n",
        "$t^n = argmax_{t^n}= \\prod P(w_i | t_i)P(t_i | t_{i-1})$\n",
        "\n",
        "$v_t(j)  = max_i {v_{t-1}(i) *c_{i,j}*P(parlabra(w)|j)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEdMVwiuGTWL"
      },
      "source": [
        "# instalacion de dependencias previas\n",
        "!pip install conllu\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQlh6bp1ydXS"
      },
      "source": [
        "# Carga del modelo HMM previamente entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pzeo7hTud5e"
      },
      "source": [
        "# cargamos las probabilidades del modelo HMM del codigo de emision y transmision dadas en \n",
        "# procesamiento de lenguaje natural Etiquetado.ipynb\n",
        "import numpy as np\n",
        "# se crean bariables para almacenas probabilidades de emision y transision \n",
        "transitionProbdict = np.load('transitionHMM.npy', allow_pickle='TRUE').item() # allow_pickle Permitir guardar matrices de objetos utilizando encurtidos Python\n",
        "emissionProbdict = np.load('emissionHMM.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYx-3Gcb4-cu"
      },
      "source": [
        "#  muestra las probabilidades condicionales\n",
        "transitionProbdict\n",
        "# emissionProbdict\n",
        "# con esto ya hemos cargado el modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As7cTF4Kyg90"
      },
      "source": [
        "# identificamos las categorias gramaticales 'upos' unicas en el corpus\n",
        "# utilizamos set para evitar que se repitan las categorias gramaticales\n",
        "stateSet = set([w.split('|')[1] for w in list(emissionProbdict.keys())]) # escogemos w.split('|') para solo selecional la categoria gramatical de cada uno de los elementos\n",
        "stateSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZb4-U_QyhFA"
      },
      "source": [
        "# enumeramos las categorias con numeros para asignar a \n",
        "# las columnas de la matriz de Viterbi\n",
        "\n",
        "# esto es un diccionario con relaciones llave valor donde las llaves \n",
        "# van ha ser las categorias gramaticales y los valores el numero de la clumna que le va a corresponder en la matriz\n",
        "tagStateDict = {} \n",
        "for i, state in enumerate(stateSet): \n",
        "  tagStateDict[state] = i\n",
        "tagStateDict\n",
        "# con esto tenemos un diccionario que relaciona cada categoria gramatical\n",
        "# con un indice que dice cual seria la fila que le corresponde dentro de la matriz de viterbi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf-eutBTynSn"
      },
      "source": [
        "# Distribucion inicial de estados latentes\n",
        "\n",
        "los estados latentes son la primera palabra que hay en cada frase dl corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTbxM1A7yhLc"
      },
      "source": [
        "# Calculamos distribución inicial de estados\n",
        "initTagStateProb = {} # \\rho_i^{(0)} creamos este diccionario que son las probabilidades de encontrar cierta categoria gramatical al inicio de la frase\n",
        "from conllu import parse_incr \n",
        "wordList = []\n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
        "count = 0 # cuenta la longitud del corpus\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  count += 1 # incrementamos el contador\n",
        "  tag = tokenlist[0]['upos'] # cojo el primer elemento con la catagoria gramatical con el upos\n",
        "  if tag in initTagStateProb.keys():\n",
        "    initTagStateProb[tag] += 1\n",
        "  else:\n",
        "    initTagStateProb[tag] = 1\n",
        "\n",
        "\n",
        "for key in initTagStateProb.keys(): \n",
        "  initTagStateProb[key] /= count # a cada elemento del dicionatio lo divido por el total de frases que hay\n",
        "\n",
        "initTagStateProb \n",
        "# esto nos devuelve las probabilidades por cada una de las categorias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhcY099865p"
      },
      "source": [
        "# para calcular la suma de las probabilidades que tiene que ser 1\n",
        "sum(initTagStateProb.values()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4-kt4MNyhSL"
      },
      "source": [
        "# verificamos que la suma de las probabilidades que deben sumar  1 (100%)\n",
        "np.array([initTagStateProb[k] for k in initTagStateProb.keys()]).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlhzJSQWy0M9"
      },
      "source": [
        "# Construccion Algoritmo de Viterbi\n",
        "\n",
        "Dada una secuencia de palabras $\\{p_1, p_2, \\dots, p_n \\}$, y un conjunto de categorias gramaticales dadas por la convención `upos`, se considera la matriz de probabilidades de Viterbi así:\n",
        "\n",
        "$$\n",
        "\\begin{array}{c c}\n",
        "\\begin{array}{c c c c}\n",
        "\\text{ADJ} \\\\\n",
        "\\text{ADV}\\\\\n",
        "\\text{PRON} \\\\\n",
        "\\vdots \\\\\n",
        "{}\n",
        "\\end{array} \n",
        "&\n",
        "\\left[\n",
        "\\begin{array}{c c c c}\n",
        "\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n",
        "\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\ \n",
        "\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n",
        "\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n",
        "p_1 & p_2 & \\dots & p_n \n",
        "\\end{array}\n",
        "\\right] \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Donde las probabilidades de la primera columna (para una categoria $i$) están dadas por: \n",
        "\n",
        "$$\n",
        "\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{probabilidad inicial}} \\times \\underbrace{P(p_1 \\vert i)}_{\\text{emisión}}\n",
        "$$\n",
        "\n",
        "luego, para la segunda columna (dada una categoria $j$) serán: \n",
        "\n",
        "$$\n",
        "\\nu_2(j) = \\max_i \\{ \\nu_1(i) \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_2 \\vert j)}_{\\text{emisión}} \\}\n",
        "$$\n",
        "\n",
        "así, en general las probabilidades para la columna $t$ estarán dadas por: \n",
        "\n",
        "$$\n",
        "\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{estado anterior}} \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_t \\vert j)}_{\\text{emisión}} \\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wtViFSAyhYg"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe98tDIQyhfO"
      },
      "source": [
        "# debemos construir la funcion porque el resultado de la funcio sera devolverme la matriz de Vitervi \n",
        "# que corresponde a la secuencia de palabras que le vamos a pasar como argumento de entrada a ala funcion\n",
        "def ViterbiMatrix(secuencia, transitionProbdict=transitionProbdict,\n",
        "                  emissionProbdict=emissionProbdict, tagStateDict=tagStateDict,\n",
        "                  initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia) # creamos el toquenizador para una secuencia toquenizada\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # 17 porque upos tiene 17 categorias, matriz(filas,columnas)\n",
        "\n",
        "  # inicialización primera columna\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key] # generemos una variable de la fila corresondiente a la etiqueta\n",
        "    word_tag = seq[0].lower()+'|'+key # asignamos de la secuencia e palabras el primer elemento y sumo la key que coresponde a al etiqueta \n",
        "    if word_tag in emissionProbdict.keys(): # creamos los elementos de la matriz\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag]\n",
        "\n",
        "  # computo de todas las probabilidades de las siguientes columnas\n",
        "  for col in range(1, len(seq)): # col para recores las columnas\n",
        "    for key in tagStateDict.keys(): # hago un for sobre cada llave o categorias gramatica\n",
        "      tag_row = tagStateDict[key] # asigno la fila relacionada con la categoria gramatical\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys(): # para ver si la etiqueta palabra esta en emission p..\n",
        "        # miramos estados de la col anterior\n",
        "        possible_probs = [] # para ello  creamos lista basia que la llenaremos con un for\n",
        "        for key2 in tagStateDict.keys(): \n",
        "          tag_row2 = tagStateDict[key2] # creamos la fila asociada a key2\n",
        "          tag_prevtag = key+'|'+key2 # consideramos las probabilidades de transiscion y eso se hace ju tando la llave actual con la anterior\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0: # aqui y en las 2 siguentes lineas consideramos la ultima parte \n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        viterbiProb[tag_row, col] = max(possible_probs)  # ahora debemos escoger el maximo de todos esos elementos\n",
        "  \n",
        "  return viterbiProb\n",
        "\n",
        "matrix = ViterbiMatrix('El mundo es pequeño')\n",
        "matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI2Fw7P-zEtd"
      },
      "source": [
        "# funcion para calcular las etiquetas\n",
        "def ViterbiTags(secuencia, transitionProbdict=transitionProbdict, emissionProbdict=emissionProbdict, \n",
        "            tagStateDict=tagStateDict, initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia)\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # upos tiene 17 categorias\n",
        "\n",
        "  # inicialización primera columna\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key]\n",
        "    word_tag = seq[0].lower()+'|'+key\n",
        "    if word_tag in emissionProbdict.keys():\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag]\n",
        "\n",
        "  # computo de las siguientes columnas\n",
        "  for col in range(1, len(seq)):\n",
        "    for key in tagStateDict.keys():\n",
        "      tag_row = tagStateDict[key]\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys():\n",
        "        # miramos estados de la col anterior\n",
        "        possible_probs = []\n",
        "        for key2 in tagStateDict.keys(): \n",
        "          tag_row2 = tagStateDict[key2]\n",
        "          tag_prevtag = key+'|'+key2\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0:\n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        viterbiProb[tag_row, col] = max(possible_probs)\n",
        "# hasta aqui repetimos el paso anterior\n",
        "\n",
        "# ahora agregaremos un paso extra donde realizaremos\n",
        "    # contruccion de secuencia de tags o etiquetas\n",
        "    res = [] # contruimos lista basia llamada res\n",
        "    for i, p in enumerate(seq): # creamos for para recorrer cada palabra de la secuencia y ademas la vamos enumerando las palabras\n",
        "      for tag in tagStateDict.keys(): #para recorrer todos los tags para que pueda ver la probabilidad de que esa palabra este asociada a lasetiquetas\n",
        "        if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n",
        "          res.append((p, tag)) # este paso es para  agregar lo anterior a la lista\n",
        "                                # aqui tendreos una lista de palabra etiqueta\n",
        "  return res\n",
        "\n",
        "ViterbiTags('el mundo es muy pequeño')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUF9Mb_hzE0t"
      },
      "source": [
        "ViterbiTags('estos instrumentos han de rasgar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15O1rz-GzKdJ"
      },
      "source": [
        "# Entrenamiento directo de HMM con NLTK\n",
        "clase en python (NLTK) de HMM: https://www.nltk.org/_modules/nltk/tag/hmm.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16XgPjiOzXRq"
      },
      "source": [
        "# Ejemplo con el Corpus Treebank en ingles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK-GYJzfzQKy"
      },
      "source": [
        "#@title ejemplo con el Corpus Treebank en ingles\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank\n",
        "train_data = treebank.tagged_sents()[:3900] # esto es asi para coger una parte de los datos para entrenamiento y la otra parte para prueba\n",
        "# y escogemos hasta la sentencia 3900 y dejamos el resto para test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmR-36Njze4a"
      },
      "source": [
        "# estructura de la data de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5GWLGyozQaj"
      },
      "source": [
        "#@title estructura de la data de entrenamiento\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht-JacXSzjML"
      },
      "source": [
        "# HMM pre-construido en NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxYRqk4RzQkh"
      },
      "source": [
        "#@title HMM pre-construido en NLTK\n",
        "from nltk.tag import hmm # cargamos clase para poder utilizar modelo\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data) # parapedir que sea un entrenamiento supervisado\n",
        "# tagger = hmm.HiddenMarkovModelTagger.train(train_data) \n",
        "tagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSGfzp_zQ2i"
      },
      "source": [
        "tagger.tag(\"Pierre Vinken will get old\".split()) # aquii estamos tokenizando con un .split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0xxmC4DzoEd"
      },
      "source": [
        "#@title training accuracy\n",
        "# para ver el porcentaje de evaluacion del modelos, osea su presicion sobre el conjunto de entrenamiento\n",
        "tagger.evaluate(treebank.tagged_sents()[:3900])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9dL9E-9zuiL"
      },
      "source": [
        "Ejercicio de práctica\n",
        "Objetivo: Entrena un HMM usando la clase hmm.HiddenMarkovModelTrainer() sobre el dataset UD_Spanish_AnCora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgsCIfBxz1fO"
      },
      "source": [
        "1. **Pre-procesamiento:** En el ejemplo anterior usamos el dataset en ingles `treebank`, el cual viene con una estructura diferente a la de `AnCora`, en esta parte escribe código para transformar la estructura de `AnCora` de manera que quede igual al `treebank` que usamos así:\n",
        "\n",
        "$$\\left[ \\left[ (\\text{'El'}, \\text{'DET'}), (\\dots), \\dots\\right], \\left[\\dots \\right] \\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzog7jgazqLA"
      },
      "source": [
        "# desarrolla tu código aquí \n",
        "# Instalamos conllu para leer el corpus\n",
        "!pip install conllu\n",
        "# Bajamos el corpus de AnCora\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git\n",
        "from conllu import parse_incr\n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\") \n",
        "\n",
        "\n",
        "# Hacemos la transformacion del corpus al formato requerido\n",
        "wordList = [] # creamos lista basia\n",
        "for tokenlist in parse_incr(data_file): \n",
        "  wordList2 = []\n",
        "  for token in tokenlist:\n",
        "    tag = token['upos']\n",
        "    valor = token['form']\n",
        "    wordList2.append((valor,tag)) \n",
        "  wordList.append(wordList2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLJhGtcz7aN"
      },
      "source": [
        "2. **Entrenamiento:** Una vez que el dataset esta con la estructura correcta, utiliza la clase `hmm.HiddenMarkovModelTrainer()` para entrenar con el $80 \\%$ del dataset como conjunto de `entrenamiento` y $20 \\%$ para el conjunto de `test`.\n",
        "\n",
        "**Ayuda:** Para la separacion entre conjuntos de entrenamiento y test, puedes usar la funcion de Scikit Learn: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "En este punto el curso de Machine Learning con Scikit Learn es un buen complemento para entender mejor las funcionalidades de Scikit Learn: https://platzi.com/cursos/scikitlearn-ml/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k9_MJCGz2jv"
      },
      "source": [
        "# desarrolla tu código aquí\n",
        "\n",
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI_AVxVoz_Dp"
      },
      "source": [
        "3. **Validación del modelo:** Un vez entrenado el `tagger`, calcula el rendimiento del modelo (usando `tagger.evaluate()`) para los conjuntos de `entrenamiento` y `test`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvmFWGbw0CN1"
      },
      "source": [
        "#desarrolla tu código aquí\n",
        "# Separamos el corpus\n",
        "wordList_train, wordList_test= train_test_split(wordList, test_size=0.20, random_state=42)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(wordList_train)\n",
        "tagger\n",
        "\n",
        "print(tagger.evaluate(wordList_test))\n",
        "print(tagger.evaluate(wordList_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrS_0dFDBnTL"
      },
      "source": [
        "Procesamiento de lenguaje natural\n",
        "kERNEL: es una funcion matematica que toma mediciones que se comportan de manera no lineal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1HZ7UGeealJ"
      },
      "source": [
        "Tenicas de clasificacion:\n",
        "\n",
        "basadas en la teoria de la probabilidad\n",
        "basadas en la teoria de la informacion\n",
        "basadas en los espacios vectoriales\n",
        "\n",
        "Clasificacion de Palabras:\n",
        "como identificacion de generon,\n",
        "etiquetado POS\n",
        "Bloqueo de palabras ofensivas\n",
        "\n",
        "Clasificacion de documento:\n",
        "analisis de sentimientos,\n",
        "topicos de coversacion,\n",
        "Priorizacion de CRMs(bases de petisiones de usarion por comunicaquin de CRM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcdOoyDfb7c"
      },
      "source": [
        "# Tareas de clasificación con NLTK\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8DBQeaxfpwD"
      },
      "source": [
        "https://www.aprendemachinelearning.com/arbol-de-decision-en-python-clasificacion-y-prediccion/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEuWXnmfk8GN"
      },
      "source": [
        "# Modelos de clasificación en Python: nombres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4kMHFGHk9ZY"
      },
      "source": [
        "import nltk, random\n",
        "nltk.download('names') # descaga data set importante\n",
        "from nltk.corpus import names # pertimte extraer esa dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEhzgjzVlxm0"
      },
      "source": [
        "Función básica de extracción de atributos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqHIum5wk9p0"
      },
      "source": [
        "# definición de atributos relevantes\n",
        "def atributos(palabra):\n",
        "\treturn {'ultima_letra': palabra[-1]}\n",
        "# escribiremos la lista de tuplas, esto contine nombres masculinos y femeninos separados en distintos archivos\n",
        "tagset = ([(name, 'male') for name in names.words('male.txt')] +\n",
        "          [(name, 'female') for name in names.words('female.txt')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVrAtER3k9vh"
      },
      "source": [
        "tagset[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef5P3qWjmhA3"
      },
      "source": [
        "# para mexclar los elementos y evitar el sesgo\n",
        "random.shuffle(tagset)\n",
        "tagset[:10] # mustrea los primeros 10 elementos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdMVZk-gmuPT"
      },
      "source": [
        "# creo una lista leyendo los elementos o atributos de la lista anterior\n",
        "# n= nombre, g = genero\n",
        "fset = [(atributos(n), g) for (n, g) in tagset]\n",
        "train, test = fset[500:], fset[:500] # divido en entrenamiento y prueba\n",
        "# digamos cogiendo los 500 en adelante para train y el resto para test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alputCutm02R"
      },
      "source": [
        "# Modelo de clasificación Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcykPvuPmuWD"
      },
      "source": [
        "# entrenamiento del modelo NaiveBayes\n",
        "classifier = nltk.NaiveBayesClassifier.train(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5buDnHBm6bD"
      },
      "source": [
        "Verificación de algunas predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Hd73temudQ"
      },
      "source": [
        "classifier.classify(atributos('camila'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQYlxpctmuiJ"
      },
      "source": [
        "classifier.classify(atributos('peter'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOihs5gKodql"
      },
      "source": [
        "# Performance del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S36l4YjXmuoL"
      },
      "source": [
        "# para calcular la metrica accuracy del modelo classifier y sobre los datos del test\n",
        "# esto nos dice el porcentaje de prediccion del modelo\n",
        "print(nltk.classify.accuracy(classifier, test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbsPVdILmuuo"
      },
      "source": [
        "# precicion del modelo sobre los datos de entrenamiento\n",
        "print(nltk.classify.accuracy(classifier, train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgJLBNxAomzv"
      },
      "source": [
        "# Mejores atributos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI_6Vq5gmu14"
      },
      "source": [
        "# para evitar escribir el alfabeto utilizamos:\n",
        "# import string\n",
        "# string.ascii_lowercase\n",
        "\n",
        "import string \n",
        "def mas_atributos(nombre):\n",
        "    atrib = {} #$ creamos diccionario basio para llenarlo con mas atributos\n",
        "    atrib[\"primera_letra\"] = nombre[0].lower() # primer caracter. y lower para convertir en minuscula\n",
        "    atrib[\"ultima_letra\"] = nombre[-1].lower() # escogemos el ultimo caracter\n",
        "    for letra in string.ascii_lowercase:\n",
        "        atrib[\"count({})\".format(letra)] = nombre.lower().count(letra) # cuenta las veces que aparece la letra\n",
        "        atrib[\"has({})\".format(letra)] = (letra in nombre.lower())  # indica si aparece como true y de lo contrario false\n",
        "    return atrib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxXF4_P2eYpS"
      },
      "source": [
        "mas_atributos('jhon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPZunCipfoQE"
      },
      "source": [
        "fset = [(mas_atributos(n), g) for (n, g) in tagset]\n",
        "train, test = fset[500:], fset[:500]\n",
        "classifier2 = nltk.NaiveBayesClassifier.train(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeZZQaTSozAd"
      },
      "source": [
        "print(nltk.classify.accuracy(classifier2, test))\n",
        "# mejoro el modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaXWBhmjuwOT"
      },
      "source": [
        "# Ejercicio de práctica\n",
        "\n",
        "Objetivo: Construye un classificador de nombres en español usando el siguiente dataset: https://github.com/jvalhondo/spanish-names-surnames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4UYgTqLu3Pu"
      },
      "source": [
        "Preparación de los datos: con un git clone puedes traer el dataset indicado a tu directorio en Colab, luego asegurate de darle el formato adecuado a los datos y sus features para que tenga la misma estructura del ejemplo anterior con el dataset names de nombres en ingles.\n",
        "Piensa y analiza: ¿los features en ingles aplican de la misma manera para los nombres en español?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYWk1gYUvBKH"
      },
      "source": [
        "# Cargamos los datasets\n",
        "!git clone https://github.com/jvalhondo/spanish-names-surnames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "995LYDJZxfNb"
      },
      "source": [
        "import numpy as np\n",
        "tag_men = np.genfromtxt('/content/spanish-names-surnames/male_names.csv', skip_header=1, delimiter=',', dtype=('U20','i8','f8'))\n",
        "tag_women = np.genfromtxt('/content/spanish-names-surnames/female_names.csv', skip_header=1, delimiter=',', dtype=('U20','i8','f8'))\n",
        "\n",
        "# escribiremos la lista de tuplas, esto contine nombres masculinos y femeninops separados en distintos archivos\n",
        "f_set = ([(name[0],'male') for name in tag_men] +\n",
        "         [(name[0],'female') for name in tag_women]) # iniciamos selecionando la primera letra de cada nombre con name[0]\n",
        "\n",
        "import random # mexclamos las listas M y F\n",
        "random.shuffle(f_set)\n",
        "\n",
        "\n",
        "# Funcion con mejores atributos \n",
        "# entrenamos un modelo sencillo usando el mismo feature de la última letra del nombre\n",
        "def atributos2(nombre):\n",
        "    atrib = {} # creamos dicionario basio para llenarlo con mas atributos\n",
        "    atrib[\"ultima_letra\"] = nombre[-1].lower() #Ultima letra\n",
        "    atrib[\"ultimas_cuatro_letras\"] = nombre[-1:-5:-1].lower() # ultimas 4 letras\n",
        "    return atrib\n",
        "\n",
        "f_varios_atributo = [(atributos2(n), g) for (n, g) in f_set]\n",
        "\n",
        "#Usamos el 75% de los datos para train y 25% para test\n",
        "f_varios_atributo_train, f_varios_atributo_test = train_test_split(f_varios_atributo, test_size=0.25, random_state=45)\n",
        "\n",
        "#Entrenamos el modelo  y Probamos\n",
        "classifier_2 = nltk.NaiveBayesClassifier.train(f_varios_atributo_train)\n",
        "print(classifier_2.classify(atributos('Juan')))\n",
        "print(nltk.classify.accuracy(classifier_2, f_varios_atributo_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KyCnPvRVmkV"
      },
      "source": [
        "# **Clasificación de palabras (por género de nombre)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUNnMEEHVsp6"
      },
      "source": [
        "import nltk, random\n",
        "# para evitarse escribir el alfabeto mejor hacer:\n",
        "import string \n",
        "# string.ascii_lowercase\n",
        "\n",
        "nltk.download('names') # descaga data set importante\n",
        "from nltk.corpus import names  # pertimte extraer esa dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYYGD51BVzMk"
      },
      "source": [
        "**Función básica de extracción de atributos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdpD9dKgVvjA"
      },
      "source": [
        "# definición que extraera los atributos relevantes\n",
        "def atributos(palabra):\n",
        "\treturn {'ultima_letra': palabra[-1]} #extrae la ultima letra\n",
        "\n",
        "'''construimos la lista de tuplas esto se hacer recoriendo\n",
        " las palabras o nombre juntando las listas de los nombres mascuinos y femeninos'''\n",
        "tagset = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R0WHM5BV29v"
      },
      "source": [
        "tagset[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxLllP3PV5ax"
      },
      "source": [
        "random.shuffle(tagset) # mexclamos la lista para evitar sesgos\n",
        "tagset[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdkZuDUcV7pl"
      },
      "source": [
        "# creamos los atributos de los nombres\n",
        "# creo una lista lellendo los atributos de la lista anterior\n",
        "fset = [(atributos(n), g) for (n, g) in tagset]\n",
        "train, test = fset[500:], fset[:500] # separamos los datops en train y test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts9-qSy-V9jq"
      },
      "source": [
        "**Modelo de clasificación Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO45sU23V_g3"
      },
      "source": [
        "# entrenamiento del modelo NaiveBayes\n",
        "classifier = nltk.NaiveBayesClassifier.train(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj9gfGXVWBRD"
      },
      "source": [
        "**Verificación de algunas predicciones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tncp2DLSWB-W"
      },
      "source": [
        "# clasifica mos los atributos de los nombres\n",
        "classifier.classify(atributos('amanda'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaHbKTxKWMOL"
      },
      "source": [
        "classifier.classify(atributos('peter'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZgm8qZtWIux"
      },
      "source": [
        "**Performance del modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP1NlO45WTHy"
      },
      "source": [
        "#verificamos el performant  sobre todo el conjunto de test\n",
        "# esto se hace calculando o clasificando  la metrica accuracy \n",
        "print(nltk.classify.accuracy(classifier, test))\n",
        "# esto nos indica que porcentaje clasifica correctamente"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHDt2-7xWVTs"
      },
      "source": [
        "# esto clasifica la metrica de train\n",
        "print(nltk.classify.accuracy(classifier, train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mxM94XBWYsx"
      },
      "source": [
        " **Mejores atributos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XDcQbwHWgEB"
      },
      "source": [
        "# definiremos mejores atributos donde el argumento de entreada sera nombre\n",
        "\n",
        "\n",
        "\n",
        "def mas_atributos(nombre):\n",
        "    atrib = {} # creamos diccionario basio el cual lo llenaremos con mas atributos\n",
        "    atrib[\"primera_letra\"] = nombre[0].lower() # primer caracter. y lower para convertir en minuscula\n",
        "    atrib[\"ultima_letra\"] = nombre[-1].lower() # escogemos el ultimo caracter\n",
        "    for letra in string.ascii_lowercase:\n",
        "        atrib[\"count({})\".format(letra)] = nombre.lower().count(letra) # cuenta el numero de bese que aparece la letra\n",
        "        # format me permite pasar letra dentro del srting  que seria como la llave de ese diccionario\n",
        "        atrib[\"has({})\".format(letra)] = (letra in nombre.lower()) # pregunta si tiene o no tiene la letra\n",
        "    return atrib # para que me retorna la lista de atributos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e60Ea-mLWjFo"
      },
      "source": [
        "mas_atributos('jhon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKtjb84OWlWS"
      },
      "source": [
        "# armamos nuestra lista de atributos\n",
        "fset = [(mas_atributos(n), g) for (n, g) in tagset]\n",
        "train, test = fset[500:], fset[:500]\n",
        "classifier2 = nltk.NaiveBayesClassifier.train(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua1MsIMKWnSx"
      },
      "source": [
        "print(nltk.classify.accuracy(classifier2, test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbs1miTZaw7m"
      },
      "source": [
        "ejercicio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK1xnunoMjyo"
      },
      "source": [
        "Objetivo: Construye un classificador de nombres en español usando el siguiente dataset: https://github.com/jvalhondo/spanish-names-surnames\n",
        "\n",
        "\n",
        "Preparación de los datos: con un git clone puedes traer el dataset indicado a tu directorio en Colab, luego asegurate de darle el formato adecuado a los datos y sus features para que tenga la misma estructura del ejemplo anterior con el dataset names de nombres en ingles.\n",
        "Piensa y analiza: ¿los features en ingles aplican de la misma manera para los nombres en español?\n",
        "\n",
        "Entrenamiento y performance del modelo: usando el classificador de Naive Bayes de NLTK entrena un modelo sencillo usando el mismo feature de la última letra del nombre, prueba algunas predicciones y calcula el performance del modelo.\n",
        "\n",
        "Mejores atributos: Define una función como atributos2() donde puedas extraer mejores atributos con los cuales entrenar una mejor version del clasificador. Haz un segundo entrenamiento y verifica como mejora el performance de tu modelo. ¿Se te ocurren mejores maneras de definir atributos para esta tarea particular?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcwvRubdWpbG"
      },
      "source": [
        "# Cargamos los datasets\n",
        "!git clone https://github.com/jvalhondo/spanish-names-surnames\n",
        "import numpy as np\n",
        "tag_men = np.genfromtxt('/content/spanish-names-surnames/male_names.csv', skip_header=1, delimiter=',', dtype=('U20','i8','f8'))\n",
        "tag_women = np.genfromtxt('/content/spanish-names-surnames/female_names.csv', skip_header=1, delimiter=',', dtype=('U20','i8','f8'))\n",
        "\n",
        "f_set = [(name[0],'male') for name in tag_men] + [(name[0],'female') for name in tag_women]\n",
        "import random\n",
        "random.shuffle(f_set)\n",
        "\n",
        "# Funcion con mejores atributos \n",
        "def atributos2(nombre):\n",
        "    atrib = {}\n",
        "    atrib[\"ultima_letra\"] = nombre[-1].lower() #Ultima letra\n",
        "    atrib[\"ultimas_dos_letra\"] = nombre[-1:-5:-1].lower() #ultimas 4 letras\n",
        "    return atrib\n",
        "\n",
        "f_varios_atributo = [(atributos2(n), g) for (n, g) in f_set]\n",
        "\n",
        "#Usamos el 80% de los datos para train y 20% para test\n",
        "f_varios_atributo_train, f_varios_atributo_test = train_test_split(f_varios_atributo, test_size=0.20, random_state=45)\n",
        "\n",
        "#Entrenamos y Probamos\n",
        "classifier_2 = nltk.NaiveBayesClassifier.train(f_varios_atributo_train)\n",
        "print(classifier_2.classify(atributos('Juan')))\n",
        "print(nltk.classify.accuracy(classifier_2, f_varios_atributo_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05LOqrjSNBjX"
      },
      "source": [
        "# **Clasificación de documentos (email spam o no spam)**\n",
        "\n",
        "\n",
        "https://github.com/pachocamacho1990/datasets: contiene una carpeta email, que contiene datos y unos corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHrYj8eoNLYo"
      },
      "source": [
        "!git clone https://github.com/pachocamacho1990/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKA3bzucNMF3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "nltk.download('punkt') # punkt(puntualizacion) es un tokenizador estandar\n",
        "nltk.download('averaged_perceptron_tagger') # average_perceptron  es un  tagger o etiquetador por defecto en ingles \n",
        "from nltk import word_tokenize # TOKENIZADOR DE PALABRAS basado en el tokenizador punkt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZVaCniGNMM3"
      },
      "source": [
        "# importamos dataframe, como este archivo no tiene nombres de columnas por lo que se le daran nombres\n",
        "# name donde la primera columnas es clase que indicara categorias si es o no es spam donde -1 es spam y contenido esla otra columna\n",
        "df = pd.read_csv('datasets/email/csv/spam-apache.csv', names = ['clase','contenido'])\n",
        "# para clasificar primero debemos separar en tokens y luego definir atributos apartir de los tokens\n",
        "df['tokens'] = df['contenido'].apply(lambda x: word_tokenize(x)) # aqui creamos nueva columna\n",
        "# esta funcion lo que esta haciendo es aplicar un word_tokenize a cada una delas filas de la columna contenido\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vii_axEONMRe"
      },
      "source": [
        "df['tokens'].values[0] # pedimos los valores de la coluna tokens y de esos valores dame el primero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78lDOrUFNaWf"
      },
      "source": [
        "# con FreqDist que es una distribucion de frecuencias que lo que hace es que para cada\n",
        "# palabra que le pase como argumento de esta funcion determian cuantasveces aparece esta palabra\n",
        "all_words = nltk.FreqDist([w for tokenlist in df['tokens'].values for w in tokenlist]) # cresmos lista con un doble for dentro\n",
        "# all_words #  muestra cada palabra con el numero de beses que aparecio\n",
        "top_words = all_words.most_common(200) # para buscar es las mas frecuentes palabras con most_common\n",
        "\n",
        "# definimos funcionpara extraer atributos de un documento\n",
        "def document_features(document):\n",
        "    document_words = set(document) # cogemos el documento y le pasamos la funcion set que lo que hace es que si hay palaras repetidas escoge solo las palabras unicas\n",
        "    features = {} # definimos un diccionario de atributos\n",
        "    for word in top_words: # para recores las  palabras m,as popilares\n",
        "        features['contains({})'.format(word[0])] = (word[0] in document_words) # por medio de format le paso una variable externa al string que aqui sera  word\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP9tIy3FNMWi"
      },
      "source": [
        "document_features(df['tokens'].values[0]) # probamos selecionando tokens y escojamos los valores y de estos la primera fila   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jQCroA8NdqZ"
      },
      "source": [
        "# construiresmos modelo de clasificacion\n",
        " #para el for texto y clase primero cogera los textos de la columna tokens del dataframe y las clases del dataframe\n",
        "fset = [(document_features(texto), clase) for texto, clase in zip(df['tokens'].values, df['clase'].values)]\n",
        "random.shuffle(fset) # mexclamos para evitar sesgos\n",
        "train, test = fset[:200], fset[200:] # dividimos en train y test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTBjHHhKNdv4"
      },
      "source": [
        "# aplicamos modelo con nltk para entrenar modelo\n",
        "classifier = nltk.NaiveBayesClassifier.train(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDNT-h4aNd27"
      },
      "source": [
        "#verificamos el performant  sobre todo el conjunto de test\n",
        "# esto se hace calculando o clasificando  la metrica accuracy \n",
        "print(nltk.classify.accuracy(classifier, test))\n",
        "# esto nos indica que % se clasifica correctamente"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5RqpyJlNjn3"
      },
      "source": [
        "classifier.show_most_informative_features(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WG7QDkmNlsn"
      },
      "source": [
        "# filtramos el dataframe  de la columna clase donde sea spam y miramos el contenido\n",
        "df[df['clase']==-1]['contenido']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdM-o8kmdLRS"
      },
      "source": [
        "reto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyFKHIyacNVG"
      },
      "source": [
        "¿Como podrías construir un mejor clasificador de documentos?\n",
        "\n",
        "Dataset más grande: El conjunto de datos que usamos fue muy pequeño, considera usar los archivos corpus que estan ubicados en la ruta: datasets/email/plaintext/\n",
        "\n",
        "Limpieza: como te diste cuenta no hicimos ningun tipo de limpieza de texto en los correos electrónicos. Considera usar expresiones regulares, filtros por categorias gramaticales, etc ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc1IvBXMdRhn"
      },
      "source": [
        "# !git clone https://github.com/pachocamacho1990/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApKi6vfEdNTu"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import random\n",
        "from nltk import word_tokenize\n",
        "from nltk.collocations import *\n",
        "import pandas as pd\n",
        "nltk.download(\"punkt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPimEa8PdUCQ"
      },
      "source": [
        "# Funciones para cargar los datasets\n",
        "\n",
        "# Get Text and labels from folders with plain text files\n",
        "from os import listdir\n",
        "from zipfile import ZipFile\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopword = stopwords.words('english')\n",
        "\n",
        "corp_path = '/content/datasets/email/plaintext/'\n",
        "files_path = ['{}'.format(corp_path) + f for f in listdir (corp_path)]\n",
        "\n",
        "df = pd.DataFrame(columns=[\"clase\", \"token\"])\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w+.]+\")\n",
        "\n",
        "for folder in files_path:\n",
        "  zf = ZipFile(folder)\n",
        "  files = [f for f in ZipFile.namelist(ZipFile(folder)) if f.endswith('.txt')]\n",
        "  for i, file_name in enumerate(files):\n",
        "    spam_ham = -1 if file_name.endswith('spam.txt') else 1\n",
        "    read = zf.open(file_name).read().decode(\"ISO-8859-1\").lower()\n",
        "    tokens = tokenizer.tokenize(read)\n",
        "    token_free = [word for word in tokens if word not in stopword]\n",
        "    datos = {'clase': spam_ham, 'token': token_free}\n",
        "    df = df.append(datos, ignore_index=True)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9oDxygUgO5j"
      },
      "source": [
        "# Descomprimir ZIP\n",
        "import zipfile\n",
        "fantasy_zip = zipfile.ZipFile('/content/datasets/email/plaintext/corpus1.zip')\n",
        "fantasy_zip.extractall('/content/datasets/email/plaintext')\n",
        "fantasy_zip.close()\n",
        "\n",
        "# Creamos un listado de los archivos dentro del Corpus1 ham/spam\n",
        "from os import listdir\n",
        "\n",
        "path_ham = \"/content/datasets/email/plaintext/corpus1/ham/\"\n",
        "filepaths_ham = [path_ham+f for f in listdir(path_ham) if f.endswith('.txt')]\n",
        "\n",
        "path_spam = \"/content/datasets/email/plaintext/corpus1/spam/\"\n",
        "filepaths_spam = [path_spam+f for f in listdir(path_spam) if f.endswith('.txt')]\n",
        "\n",
        "# Creamos la funcion para tokenizar y leer los archivos \n",
        "\n",
        "def abrir(texto):\n",
        "  with open(texto, 'r', errors='ignore') as f2:\n",
        "    data = f2.read()\n",
        "    data = word_tokenize(data)\n",
        "  return data\n",
        "\n",
        "# Creamos la lista tokenizada del ham\n",
        "list_ham = list(map(abrir, filepaths_ham))\n",
        "# Creamos la lista tokenizada del spam\n",
        "list_spam = list(map(abrir, filepaths_spam))\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Separamos las palabras mas comunes\n",
        "all_words = nltk.FreqDist([w for tokenlist in list_ham+list_spam for w in tokenlist])\n",
        "top_words = all_words.most_common(250)\n",
        "\n",
        "# Agregamos Bigramas\n",
        "bigram_text = nltk.Text([w for token in list_ham+list_spam for w in token])\n",
        "bigrams = list(nltk.bigrams(bigram_text))\n",
        "top_bigrams = (nltk.FreqDist(bigrams)).most_common(250)\n",
        "\n",
        "\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    bigram = set(list(nltk.bigrams(nltk.Text([token for token in document]))))\n",
        "    features = {}\n",
        "    for word, j in top_words:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "\n",
        "    for bigrams, i in top_bigrams:\n",
        "        features['contains_bigram({})'.format(bigrams)] = (bigrams in bigram)\n",
        "  \n",
        "    return features\n",
        "\n",
        "# Juntamos las listas indicando si tienen palabras de las mas comunes\n",
        "import random\n",
        "fset_ham = [(document_features(texto), 0) for texto in list_ham]\n",
        "fset_spam = [(document_features(texto), 1) for texto in list_spam]\n",
        "fset = fset_spam + fset_ham[:1500]\n",
        "random.shuffle(fset)\n",
        "\n",
        "# Separamos en las listas en train y test\n",
        "from sklearn.model_selection import train_test_split\n",
        "fset_train, fset_test = train_test_split(fset, test_size=0.20, random_state=45)\n",
        "\n",
        "# Entrenamos el programa\n",
        "classifier = nltk.NaiveBayesClassifier.train(fset_train)\n",
        "\n",
        "# Probamos y calificamos\n",
        "classifier.classify(document_features(list_ham[34]))\n",
        "print(nltk.classify.accuracy(classifier, fset_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}