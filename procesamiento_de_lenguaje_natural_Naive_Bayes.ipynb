{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "procesamiento de lenguaje natural Naive Bayes.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM82YuAW+SUSeRds4w0othy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WuilsonEstacio/Procesamiento-de-lenguaje-natural/blob/main/procesamiento_de_lenguaje_natural_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk_FjttHkKEk"
      },
      "source": [
        "Un NB clasificador  probabilistico es aque que dado un documeto cualquiera le asiga una probabilidad para cada una de las categorias en su interior\n",
        "\n",
        "$c= arg max_c * \\dfrac{P(d/c)P(c}{P(d)}= arg max_c *{P(d/c)P(c)}$\n",
        "\n",
        "$P(d/c)= P(f1,f2,f3... fn | c)=P(f1/c)P(f2/c)...P(fn/c) $ es ta es la probabilidad de que dada una categoria encontraramos un documento, fi son features o atributos\n",
        "\n",
        "$c= arg max_c *P(c)\\prod_i^n P(fi/c) $\n",
        "aplicaremos un log para evitar que el numero se valla a cero a infinito por lo tanto\n",
        "\n",
        "$c= arg max_c*Log(P(c)\\prod_i^n P(fi/c))$\n",
        "\n",
        "$c= arg max_c*Log(P(c))+\\sum Log(P(fi/c)) $\n",
        "\n",
        "notya: underflow: Sobrepasar la precisión de máquina con un espacio logaritmico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6muqLOH2ha-c"
      },
      "source": [
        "import math\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDo3fc-BoPFw"
      },
      "source": [
        "!git clone https://github.com/pachocamacho1990/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_xnWDzocHN"
      },
      "source": [
        "! unzip datasets/email/plaintext/corpus1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g48E_cUqoLEi"
      },
      "source": [
        "os.listdir('corpus1/spam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MF8QDjnojQe"
      },
      "source": [
        "data = []\n",
        "clases = [] # esta lista nos dice si la etiqueta es spam o no lo es \n",
        "\n",
        "# lectura de spam datal, pare ello usamos la libreria  os. para recorres una lista de archivos \n",
        "for file in os.listdir('corpus1/spam'): # usamos el comando listdir para listar los archivos que encuentre en el directorio  corpus1/spam\n",
        "  # ahora abrimos cada interaccion del for. encoding='latin-1'  por si se encuentran caracteres especiales\n",
        "  with open('corpus1/spam/'+file, encoding='latin-1') as f:\n",
        "    data.append(f.read()) # a la lista data le agrego el texto del archivo\n",
        "    clases.append('spam') # le agrego la clase de ese archivo\n",
        "\n",
        "# realizaremos lectura de ham data\n",
        "for file in os.listdir('corpus1/ham'):\n",
        "  with open('corpus1/ham/'+file, encoding='latin-1') as f:\n",
        "    data.append(f.read())\n",
        "    clases.append('ham')\n",
        "len(data) # nos muestra la longitud de data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maUn86F8omtH"
      },
      "source": [
        "# **Construcción de modelo Naive Bayes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcuePCeMoogV"
      },
      "source": [
        "# Tokenizador de Spacy\n",
        "Documentación: <a href= https://spacy.io/api/tokenizer >Spacy</a> Esto permite implementar flujos de procesamiento, para llevar modelos de procesamiento de lenguaje a produccion\n",
        "\n",
        "¿Cómo funciona el tokenizador? <a href=https://spacy.io/usage/linguistic-features#how-tokenizer-works>tokenizador_doc_spacy</a> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7bc9dN0rSUV"
      },
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "tokenizer = Tokenizer(nlp.vocab) # vocab es Un contenedor de almacenamiento para tipos léxicos.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3xnd2LGrSat"
      },
      "source": [
        "print([t.text for t in tokenizer(data[0])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piUbHI0grXjm"
      },
      "source": [
        "Clase principal para el algoritmo\n",
        "Recuerda que la clase más probable viene dada por (en espacio de cómputo logarítmico):\n",
        "\n",
        "$c^=argmax(c)logP(c)+∑i=1nlogP(fi|c)$ \n",
        "\n",
        "Donde, para evitar casos atípicos, usaremos el suavizado de Laplace así:\n",
        "\n",
        "$P(fi|c)=\\dfrac{C(fi,c)+1}{ C(c)+|V|}$\n",
        "\n",
        "siendo $ |V|$  la longitud del vocabulario de nuestro conjunto de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wnzlDI1rSgK"
      },
      "source": [
        "# construccion del algoritmo\n",
        "import numpy as np\n",
        "\n",
        "class NaiveBayesClassifier(): # construccion de la clase \n",
        "  nlp = English() # atributos de la clase\n",
        "  tokenizer = Tokenizer(nlp.vocab) # atributos de la clase\n",
        "  \n",
        "  # retorna el documento tokenizado en una lista\n",
        "  def tokenize(self, doc): \n",
        "    return  [t.text.lower() for t in tokenizer(doc)]\n",
        "\n",
        "# contador de palabras\n",
        "  def word_counts(self, words):\n",
        "    wordCount = {} # diccionario basio\n",
        "    for w in words: # recorro tofdas las palabras\n",
        "      if w in wordCount.keys(): # verificamos si la palabra esta en el diccionario\n",
        "        wordCount[w] += 1 # si lo anterior sucede le sumamos una llave adecional\n",
        "      else:\n",
        "        wordCount[w] = 1 # de lo contrario creamos la llave\n",
        "    return wordCount # debemos retornar un conteo\n",
        "\n",
        "# ajustamos el modelo para poder entrenarlo y calcular todas sus probabilidades\n",
        "  def fit(self, data, clases):\n",
        "    n = len(data) # definimos la longitud de los datos de entrenamiento\n",
        "    self.unique_clases = set(clases) # identificamos clases unicas\n",
        "    self.vocab = set() # creamos atributo para asignar el vocabulario\n",
        "    self.classCount = {} # C(c) Creamos una clase para contar el numero de bese que aparece cada categoria posible en el corpus\n",
        "    self.log_classPriorProb = {} #P(c) calculamos el log de la probabilidad de las clases\n",
        "    self.wordConditionalCounts = {} #C(w|c) probabilidad de las clases condicionales\n",
        "   \n",
        "    #Conteos de clases\n",
        "    for c in clases: # repetimos lo de arriba\n",
        "      if c in self.classCount.keys():\n",
        "        self.classCount[c] += 1\n",
        "      else:\n",
        "        self.classCount[c] = 1\n",
        "\n",
        "    # Calculo de P(c)\n",
        "    for c in self.classCount.keys():\n",
        "      self.log_classPriorProb[c] = math.log(self.classCount[c]/n)\n",
        "      self.wordConditionalCounts[c] = {}\n",
        "\n",
        "    # calculo de C(w|c)\n",
        "    for text, c in zip(data, clases):\n",
        "       # por cada interacion del for se hace un conteo del texto tokenizado\n",
        "      counts = self.word_counts(self.tokenize(text))\n",
        "      for word, count in counts.items(): # este for es para contar el numero de veces que aparece cada palabra\n",
        "        if word not in self.vocab: # verificamos que la palabra no este en el diccionario  self.vocab.add(word)\n",
        "          self.vocab.add(word) # y si no esta la agregamos\n",
        "        if word not in self.wordConditionalCounts[c]: # si la palabra no esta en los conteos condicionales\n",
        "          self.wordConditionalCounts[c][word] = 0.0 # si word no esta alli pues agregamos el elemento de la llave\n",
        "        self.wordConditionalCounts[c][word] += count\n",
        "        \n",
        "# para poder predecir las clases\n",
        "  def predict(self, data):\n",
        "    results = [] # creamos lista vacia para almacenar\n",
        "    for text in data:\n",
        "      words = set(self.tokenize(text)) #\n",
        "      scoreProb = {} # definimos diccionario donde cada clase tiene su probabilidad\n",
        "      for word in words: \n",
        "        if word not in self.vocab: continue #esti nos dice que si la palabra no esta en el diccionario entonces la ignoramos\n",
        "       # si la palabra si esta en el diccionario entonces la suabizamos\n",
        "        #suavizado Laplaciano para P(w|c)\n",
        "        for c in self.unique_clases:\n",
        "          log_wordClassProb = math.log(\n",
        "              (self.wordConditionalCounts[c].get(word, 0.0)+1)/(self.classCount[c]+len(self.vocab)))\n",
        "          scoreProb[c] = scoreProb.get(c, self.log_classPriorProb[c]) + log_wordClassProb\n",
        "      arg_maxprob = np.argmax(np.array(list(scoreProb.values())))\n",
        "      '''en la sig linea agregamos la maxima probabilidad correspondiente a ese argumento\n",
        "      esto se hace con scoreProb.keys() donde cogemos la lista y de esa lista cogemos la posiscion \n",
        "      donde al agumento me dio la maxima probabilidad'''\n",
        "      results.append(list(scoreProb.keys())[arg_maxprob])\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJiR9dbZrwtG"
      },
      "source": [
        "Utilidades de Scikit Learn\n",
        "train_test_split: <a href = https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html>AQUI</a>\n",
        "\n",
        "accuracy_score: <a href =https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html>AQUI</a>\n",
        "\n",
        "precision_score:<a href = https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html>AQUI</a>\n",
        "\n",
        "recall_score: <a href = https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html>AQUI</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PzkChPx9Jg6"
      },
      "source": [
        "**Verificaremos si el algoritmo funciona correctamente**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4DCrIctruNK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "data_train, data_test, clases_train, clases_test = train_test_split(data, clases, test_size=0.10, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPryHR2rsFhP"
      },
      "source": [
        "classifier = NaiveBayesClassifier()\n",
        "classifier.fit(data_train, clases_train) # ajustamos los datos y clases de entrenamiento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmuZv48AsFnm"
      },
      "source": [
        "clases_predict = classifier.predict(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U88E7PWksFt6"
      },
      "source": [
        "# Acurracy\n",
        "accuracy_score(clases_test, clases_predict) # accuracy_score lo usamos para comparar las clases del test, encomparacion con las clases predichas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liawj4cwsK9A"
      },
      "source": [
        "# precision\n",
        "# esto indica que cuanto de los selecionados como  ham y spam son verdaderamente spam\n",
        "# precicion (ham, spam)\n",
        "precision_score(clases_test, clases_predict, average=None, zero_division=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8fCTOT1sLC9"
      },
      "source": [
        "# recall\n",
        "# esto nos indica cuantos datos de ham y spam logramosidentificar bien\n",
        "# recall(ham,spam)\n",
        "recall_score(clases_test, clases_predict, average=None, zero_division=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB0gzWdr_y7b"
      },
      "source": [
        "$Accuracy(y,y')=\\dfrac{1}{n} \\sum_{i=1}^n \\Upsilon_{y_i,y_i´}$\n",
        "\n",
        "el acurracy es la proporcion entre el numero de prediccions que fueron correctas y el numero total de predecciones realizadas si \n",
        "esta es =1 si son iguales\n",
        "\n",
        "Acurracy = TP+ TN\n",
        "\n",
        "\n",
        "$Precicion = \\dfrac{TP}{TP+FP}$ esto indica que cuanto de los selecionados como x son verdaderamente x\n",
        "\n",
        "\n",
        "$Recall=\\dfrac{TP}{TP+FN}$ esto nos indica cuantos datos x logramosidentificar bien"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Rc7nf4N5r2"
      },
      "source": [
        "# **Test** \n",
        "\n",
        "1.\n",
        "Si tenemos un dataset etiquetado donde la categoría adjetivo (ADJ) aparece un total de 500 veces entre todos los tokens, y de esas veces solamente la palabra \"noble\" le corresponde 200 veces, entonces podemos decir que:\n",
        "\n",
        "La probabilidad de emisión P(noble|ADJ) = 40%\n",
        "\n",
        "2.\n",
        "El proceso mediante el cual un Modelo Markoviano Latente determina la secuencia de etiquetas más probable para una secuencia de palabras es:\n",
        "\n",
        "Usando el algoritmo de Viterbi para obtener la categoría más probable, palabra por palabra.\n",
        "\n",
        "3.\n",
        "Dada una cadena de texto text en español, el procedimiento para asignar las etiquetas gramaticales con Stanza es a partir de un objeto nlp(text), donde:\n",
        "\n",
        "nlp = stanza.Pipeline('es', processors='tokenize,pos')\n",
        "\n",
        "4.\n",
        "La ingeniería de atributos se usa para:\n",
        "\n",
        "Construir atributos particulares de palabras y textos que permitan dar un input más apropiado a un modelo de clasificación.\n",
        "\n",
        "5.\n",
        "El problema de clasificación de texto pertenece a la categoría de Machine Learning supervisado porque:\n",
        "\n",
        "Durante el entrenamiento, el modelo tiene conocimiento de las etiquetas correctas que debería predecir.\n",
        "\n",
        "6.\n",
        "En un modelo de clasificación por categorías gramaticales, el algoritmo de Viterbi se usa para:\n",
        "\n",
        "El proceso de decodificación: encontrar la secuencia de etiquetas más probable.\n",
        "\n",
        "7.\n",
        "En un Modelo Markoviano Latente se necesitan los siguientes ingredientes:\n",
        "\n",
        "Matrices de transición, emisión y distribución inicial de estados.\n",
        "\n",
        "8.\n",
        "En un problema de clasificación de emails entre SPAM y HAM, la métrica de recall tiene la siguiente interpretación:\n",
        "\n",
        "De todos los correos que realmente son SPAM, la fracción que el modelo logró identificar.\n",
        "\n",
        "9.\n",
        "Para entrenar un clasificador de Naive Bayes en NLTK, se escribe en Python:\n",
        "\n",
        "nltk.NaiveBayesClassifier.train(data)\n",
        "\n",
        "10.\n",
        "Si tienes un modelo de clasificación binaria que luego de entrenarlo, obtienes que el número de verdaderos positivos es 200 y el número de falsos positivos es 120, entonces la métrica de precisión de dicho modelo tiene un valor de:\n",
        "\n",
        "200/320\n",
        "\n",
        "11.\n",
        "Un algoritmo general de clasificación de texto:\n",
        "\n",
        "Es un algoritmo de Machine Learning supervisado.\n",
        "\n",
        "12.\n",
        "El tokenizador por defecto en NLTK para el idioma inglés es:\n",
        "\n",
        "punkt\n",
        "\n",
        "13.\n",
        "En una cadena de Markov se necesitan los siguientes elementos:\n",
        "\n",
        "Matriz de transiciones y distribución inicial de estados.\n",
        "\n",
        "14.\n",
        "Entrenar un Modelo Markoviano Latente significa:\n",
        "\n",
        "Calcular las matrices de probabilidad de transición y emisión con un corpus de textos.\n",
        "\n",
        "15.\n",
        "Una de las siguientes no es una categoría de ambigüedades del lenguaje:\n",
        "\n",
        "Vectorial\n",
        "\n",
        "16.\n",
        "El suavizado de Laplace se usa en un algoritmo de clasificación con el objetivo de:\n",
        "\n",
        "Evitar probabilidades nulas y denominadores iguales a cero.\n",
        "\n",
        "17.\n",
        "El clasificador de Naive Bayes es:\n",
        "\n",
        "Un clasificador probabilístico que hace uso de la regla de Bayes.\n",
        "\n",
        "18.\n",
        "En la frase: \"mi hermano es muy noble\", la palabra noble hace referencia a:\n",
        "\n",
        "Un adjetivo\n",
        "\n",
        "19.\n",
        "Con Naive Bayes preferimos hacer cálculos en espacio logarítmico para:\n",
        "\n",
        "Evitar productos de números demasiado pequeños para la precisión de máquina.\n",
        "\n",
        "20.\n",
        "En un modelo MEMM:\n",
        "\n",
        "El proceso de decodificación es similar al de un HMM, y por lo tanto se puede usar un tipo de algoritmo de Viterbi.\n",
        "\n",
        "21.\n",
        "El accuracy de entrenamiento de un modelo se calcula como:\n",
        "\n",
        "(número de veces que el modelo predice la categoría correcta) / (total de datos usados para entrenamiento)\n",
        "\n",
        "22.\n",
        "Si tenemos una cadena de Markov para describir las probabilidades de transición en cuanto al clima de un dia para otro, y observamos la siguiente secuencia de estados día tras día: (frío, frío, caliente, frío, tibio, caliente, tibio, frío), entonces la probabilidad de transición P(caliente|frío) es:\n",
        "\n",
        "50%\n",
        "\n",
        "23.\n",
        "\n",
        "En un Modelo Markoviano Latente, el problema de calcular la secuencia de etiquetas más probable se expresa con la siguiente expresión matemática:\n",
        "\n",
        "$${\\arg \\max}_{(t^n)}\\prod_i P(w_i \\vert t_i)P(t_i \\vert t_{i-1})$$\n",
        "\n",
        "24.\n",
        "Para un modelo de clasificación de palabras con Naive Bayes en NLTK, debemos entrenar el algoritmo usando:\n",
        "\n",
        "nltk.NaiveBayesClassifier.train(train_set) donde usamos una funcion que extrae atributos llamada atributos() y:\n",
        "\n",
        "train_set = [(atributos(palabra), categoría de la palabra), ...]\n",
        "\n",
        "25.\n",
        "Dada una cadena de texto text en inglés, el procedimiento para asignar las etiquetas gramaticales con NLTK es:\n",
        "\n",
        "nltk.pos_tag(word_tokenize(text))"
      ]
    }
  ]
}